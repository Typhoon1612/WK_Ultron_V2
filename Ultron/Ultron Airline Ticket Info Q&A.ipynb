{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5790c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"what\" in query_lemmas and \"is\" in query_lemmas:\n",
    "                if not names:\n",
    "                    return \"I'm sorry, I didn't catch your name. Can you please tell me your name?\"\n",
    "            else:\n",
    "                return responses[\"my_name\"][0].format(name=names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e42c0ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Ultron Airline customer service chatbot!\n",
      "You: I am from Malaysia\n",
      "Chatbot: Yes, we have flight that are reaching Malaysia\n",
      "['book', 'reserve', 'hold', 'book', 'book', 'book', 'buy', 'purchase', 'bribe', 'corrupt', 'buy', \"grease_one's_palms\", 'buy', 'buy', 'buy']\n",
      "You: My name is Jim\n",
      "Chatbot: I'm sorry, I didn't understand your query. Please try again.\n",
      "['book', 'reserve', 'hold', 'book', 'book', 'book', 'buy', 'purchase', 'bribe', 'corrupt', 'buy', \"grease_one's_palms\", 'buy', 'buy', 'buy']\n",
      "You: Hello, my name is Jim\n",
      "Chatbot: Hello Jim\n",
      "['book', 'reserve', 'hold', 'book', 'book', 'book', 'buy', 'purchase', 'bribe', 'corrupt', 'buy', \"grease_one's_palms\", 'buy', 'buy', 'buy']\n",
      "You: Do you fly to China?\n",
      "Chatbot: Sorry, currently, we are only supporting Asean Country!!!\n",
      "['book', 'reserve', 'hold', 'book', 'book', 'book', 'buy', 'purchase', 'bribe', 'corrupt', 'buy', \"grease_one's_palms\", 'buy', 'buy', 'buy']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a list of possible responses to customer queries\n",
    "responses = {\n",
    "    \"greeting\": [\"Hello, how may I assist you?\", \"Welcome, how can I help you today?\"],\n",
    "    \"booking\": [\"You may proceed to UltronAirline.com for booking!\"],\n",
    "    #notyet\n",
    "    \"support_channel\": [\"You can contact our support team at support@example.com or by calling 1-800-123-4567.\"],\n",
    "    \"available-country\": [\"\\nOur flights only operate within Southeast Asian countries.\\nWhich only included:\\n1. Brunei\\n2. Cambodia\\n3. Indonesia\\n4. Laos\\n5. Malaysia\\n6. Myanmar\\n7. Philippines\\n8. Singapore\\n9. Thailand\\n10. Timor-Leste\\n11. Vietnam\"],\n",
    "    \"country_name\": [\"Yes, we have flight that are reaching {country_name}\"],\n",
    "    \"not_understand\": [\"Sorry, currently, we are only supporting Asean Country!!!\", \"I'm sorry, I didn't understand your query. Please try again.\"],\n",
    "    \"packages\": [\"2 Way Tickets\", \"1 Way Tickets\"],\n",
    "    \"user_Intro\": [\"Hello {name}\"],\n",
    "    \"my_name\": [\"Your name is {name}\"]\n",
    "}\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "name = \"\"\n",
    "asean_countries = {'Brunei', 'Cambodia', 'Indonesia', 'Laos', 'Malaysia', 'Myanmar', 'Philippines', 'Singapore', 'Thailand', 'Vietnam'}\n",
    "\n",
    "# Define a function to lemmatize a given sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words back into a sentence\n",
    "    lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Define a function to extract names and country names from a sentence\n",
    "def extract_names_and_countries(query):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(query)\n",
    "    # Tag the words with their part of speech\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # Extract the named entities from the tagged words\n",
    "    named_entities = nltk.ne_chunk(tagged_words)\n",
    "    # Filter out non-person named entities\n",
    "    person_entities = [entity for entity in named_entities if isinstance(entity, nltk.tree.Tree) and entity.label() == \"PERSON\"]\n",
    "    # Extract the person names from the person named entities\n",
    "    person_names = [ \" \".join([word[0] for word in entity.leaves()]) for entity in person_entities]\n",
    "    # Filter out non-country named entities\n",
    "    country_entities = [entity for entity in named_entities if isinstance(entity, nltk.tree.Tree) and entity.label() == \"GPE\"]\n",
    "    # Extract the country names from the country named entities\n",
    "    country_names = [ \" \".join([word[0] for word in entity.leaves()]) for entity in country_entities]\n",
    "    return person_names, country_names\n",
    "\n",
    "def booking_synonym(query):\n",
    "    synonyms = []\n",
    "    words = [\"book\", \"buy\"]\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word, pos='v'):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "# Define a function to handle customer queries and generate responses\n",
    "def chatbot_response(query):\n",
    "    global name  # Declare global variable to modify name inside the function\n",
    "    query_lemmas = lemmatize_sentence(query.lower())\n",
    "    # Extract person names and country names from the query\n",
    "    names, country_names = extract_names_and_countries(query)\n",
    "    if \"hello\" in query_lemmas or \"hi\" in query_lemmas and (\"my\" in query_lemmas and \"name\" in query_lemmas):\n",
    "        # Check if the query includes the user's name\n",
    "        if names:\n",
    "            name = names[0]\n",
    "            return responses[\"user_Intro\"][0].format(name=name)\n",
    "        else:\n",
    "            return random.choice(responses[\"greeting\"])\n",
    "    elif \"what\" in query_lemmas and \"is\" in query_lemmas and \"my\" in query_lemmas and \"name\" in query_lemmas:\n",
    "        if name:\n",
    "            return responses[\"my_name\"][0].format(name=name)\n",
    "        else:\n",
    "            return \"I'm sorry, I didn't catch your name. Can you please tell me your name?\"\n",
    "    elif any(lemma in query_lemmas for lemma in booking_synonym(query)):\n",
    "        return responses[\"booking\"][0]\n",
    "    elif \"where\" in query_lemmas or (\"which\" in query_lemmas and \"country\"in query_lemmas):\n",
    "        return responses[\"available-country\"][0]\n",
    "    elif country_names:\n",
    "        country_name = country_names[0]\n",
    "        if country_name not in asean_countries:\n",
    "            return responses[\"not_understand\"][0].format(country_name=country_name) \n",
    "        else:\n",
    "            for country in asean_countries:\n",
    "                if country == country_name:\n",
    "                    return responses[\"country_name\"][0].format(country_name=country_name)\n",
    "    else:\n",
    "        return responses[\"not_understand\"][1]\n",
    "\n",
    "# Define a function to handle user interaction\n",
    "def chatbot():\n",
    "    print(\"Welcome to Ultron Airline customer service chatbot!\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        #print(booking_synonym(query))\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        response = chatbot_response(query)\n",
    "        print(\"Chatbot: \" + response)\n",
    "        print(booking_synonym(query))\n",
    "        #print(person_names)\n",
    "\n",
    "# Run the chatbot function\n",
    "chatbot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "51eae007",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What?How to book a tickets\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# sample text\n",
    "text = input(\"What?\")\n",
    "\n",
    "# tokenize the text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# use Part-of-Speech (POS) tagging to tag the words with their part of speech\n",
    "tagged_words = nltk.pos_tag(tokens)\n",
    "\n",
    "# use Named Entity Recognition (NER) to label the named entities in the text\n",
    "ner_tagged = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "# iterate over the entities and print the country names\n",
    "for entity in ner_tagged:\n",
    "    if hasattr(entity, 'label') and entity.label() == 'GPE':\n",
    "        if len(entity.leaves()) == 1 and entity.leaves()[0][1] == 'NNP':\n",
    "            print(entity.leaves()[0][0])\n",
    "            #print(tagged_words)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f562aca3",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConfigurationError",
     "evalue": "Using Nominatim with default or sample `user_agent` \"my-application\" is strongly discouraged, as it violates Nominatim's ToS https://operations.osmfoundation.org/policies/nominatim/ and may possibly cause 403 and 429 HTTP errors. Please specify a custom `user_agent` with `Nominatim(user_agent=\"my-application\")` or by overriding the default `user_agent`: `geopy.geocoders.options.default_user_agent = \"my-application\"`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConfigurationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28280\\187194104.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m# instantiate a geolocator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mgeolocator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNominatim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_agent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"my-application\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# tokenize the text and perform named entity recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\geopy\\geocoders\\nominatim.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, timeout, proxies, domain, scheme, user_agent, ssl_context, adapter_factory)\u001b[0m\n\u001b[0;32m    103\u001b[0m         if (self.domain == _DEFAULT_NOMINATIM_DOMAIN\n\u001b[0;32m    104\u001b[0m                 and self.headers['User-Agent'] in _REJECTED_USER_AGENTS):\n\u001b[1;32m--> 105\u001b[1;33m             raise ConfigurationError(\n\u001b[0m\u001b[0;32m    106\u001b[0m                 \u001b[1;34m'Using Nominatim with default or sample `user_agent` \"%s\" is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m                 \u001b[1;34m'strongly discouraged, as it violates Nominatim\\'s ToS '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mConfigurationError\u001b[0m: Using Nominatim with default or sample `user_agent` \"my-application\" is strongly discouraged, as it violates Nominatim's ToS https://operations.osmfoundation.org/policies/nominatim/ and may possibly cause 403 and 429 HTTP errors. Please specify a custom `user_agent` with `Nominatim(user_agent=\"my-application\")` or by overriding the default `user_agent`: `geopy.geocoders.options.default_user_agent = \"my-application\"`."
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "# sample text\n",
    "text = \"I am planning a trip to Singapore, Malaysia, and Thailand.\"\n",
    "\n",
    "# instantiate a geolocator\n",
    "geolocator = Nominatim(user_agent=\"my-application\")\n",
    "\n",
    "# tokenize the text and perform named entity recognition\n",
    "tokens = nltk.word_tokenize(text)\n",
    "tags = nltk.pos_tag(tokens)\n",
    "chunks = nltk.ne_chunk(tags)\n",
    "\n",
    "# create a set of Southeast Asian country names\n",
    "se_asia = set(['Brunei', 'Cambodia', 'Indonesia', 'Laos', 'Malaysia', 'Myanmar', 'Philippines', 'Singapore', 'Thailand', 'Timor-Leste', 'Vietnam'])\n",
    "\n",
    "# extract the country names from the named entity chunks\n",
    "countries = []\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk, 'label') and chunk.label() == 'GPE':\n",
    "        country = ' '.join(c[0] for c in chunk.leaves())\n",
    "        if country in se_asia:\n",
    "            countries.append(country)\n",
    "\n",
    "print(countries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0b2117",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16400\\3614015878.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0masean_countries\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'Brunei'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Cambodia'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Indonesia'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Laos'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Malaysia'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Myanmar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Philippines'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Singapore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Thailand'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Vietnam'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masean_countries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "asean_countries = {'Brunei', 'Cambodia', 'Indonesia', 'Laos', 'Malaysia', 'Myanmar', 'Philippines', 'Singapore', 'Thailand', 'Vietnam'}\n",
    "\n",
    "print(asean_countries[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443ae73a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want:support\n",
      "{'avail', 'serve', 'help', 'help_oneself', 'assist', 'facilitate', 'aid'}\n",
      "No\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "while True:\n",
    "    words = input(\"What do you want:\")\n",
    "    word = \"help\"\n",
    "    synonyms = []\n",
    "\n",
    "    for syn in wordnet.synsets(word, pos=['v']):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "\n",
    "    if words == \"exit\":\n",
    "        break\n",
    "    print(set(synonyms))\n",
    "    if words.lower() in set(synonyms):\n",
    "        print(\"Yes\")\n",
    "    else:\n",
    "        print(\"No\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846f9b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Define a list of possible responses to customer queries\n",
    "responses = {\n",
    "    \"greeting\": [\"Hello, how may I assist you?\", \"Welcome, how can I help you today?\"],\n",
    "    \"booking\": [\"You may proceed to UltronAirline.com for booking!\"],\n",
    "    #notyet\n",
    "    \"support_channel\": [\"You can contact our support team at support@example.com or by calling 1-800-123-4567.\"],\n",
    "    \"available-country\": [\"\\nOur flights only operate within Southeast Asian countries.\\nWhich only included:\\n1. Brunei\\n2. Cambodia\\n3. Indonesia\\n4. Laos\\n5. Malaysia\\n6. Myanmar\\n7. Philippines\\n8. Singapore\\n9. Thailand\\n10. Timor-Leste\\n11. Vietnam\"],\n",
    "    \"country_name\": [\"Yes, we have flight that are reaching {country_name}\"],\n",
    "    \"not_understand\": [\"Sorry, currently, we are only supporting Asean Country!!!\", \"I'm sorry, I didn't understand your query. Please try again.\"],\n",
    "    \"packages\": [\"2 Way Tickets\", \"1 Way Tickets\"],\n",
    "    \"user_Intro\": [\"Hello {name}\"],\n",
    "    \"my_name\": [\"Your name is {name}\"]\n",
    "}\n",
    "\n",
    "# Create a lemmatizer object\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "name = \"\"\n",
    "asean_countries = {'Brunei', 'Cambodia', 'Indonesia', 'Laos', 'Malaysia', 'Myanmar', 'Philippines', 'Singapore', 'Thailand', 'Vietnam'}\n",
    "\n",
    "# Define a function to lemmatize a given sentence\n",
    "def lemmatize_sentence(sentence):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Lemmatize each word\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    # Join the lemmatized words back into a sentence\n",
    "    lemmatized_sentence = \" \".join(lemmatized_words)\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Define a function to extract names from a sentence\n",
    "def extract_names(query):\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(query)\n",
    "    # Tag the words with their part of speech\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    # Extract the named entities from the tagged words\n",
    "    named_entities = nltk.ne_chunk(tagged_words)\n",
    "    # Filter out non-person named entities\n",
    "    person_entities = [entity for entity in named_entities if isinstance(entity, nltk.tree.Tree) and entity.label() == \"PERSON\"]\n",
    "    # Extract the person names from the person named entities\n",
    "    person_names = [ \" \".join([word[0] for word in entity.leaves()]) for entity in person_entities]\n",
    "    return person_names\n",
    "\n",
    "def extract_country_names(query):\n",
    "    # tokenize the text\n",
    "    tokens = nltk.word_tokenize(query)\n",
    "\n",
    "    # use Part-of-Speech (POS) tagging to tag the words with their part of speech\n",
    "    tagged_words = nltk.pos_tag(tokens)\n",
    "\n",
    "    # use Named Entity Recognition (NER) to label the named entities in the text\n",
    "    ner_tagged = nltk.ne_chunk(tagged_words)\n",
    "\n",
    "    # Filter out non-person named entities\n",
    "    country_entities = [entity for entity in ner_tagged if isinstance(entity, nltk.tree.Tree) and entity.label() == \"GPE\"]\n",
    "    # Extract the person names from the person named entities\n",
    "    country_names = [ \" \".join([word[0] for word in entity.leaves()]) for entity in country_entities]\n",
    "    return country_names\n",
    "\n",
    "def booking_synonym(query):\n",
    "    synonyms = []\n",
    "    words = [\"book\", \"buy\"]\n",
    "    for word in words:\n",
    "        for syn in wordnet.synsets(word, pos='v'):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "\n",
    "# Define a function to handle customer queries and generate responses\n",
    "def chatbot_response(query):\n",
    "    global name  # Declare global variable to modify name inside the function\n",
    "    # Lemmatize the query\n",
    "    query_lemmas = lemmatize_sentence(query.lower())\n",
    "    names = extract_names(query)\n",
    "    country_names = extract_country_names(query)\n",
    "    if \"hello\" in query_lemmas or \"hi\" in query_lemmas and (\"my\" in query_lemmas and \"name\" in query_lemmas):\n",
    "        # Check if the query includes the user's name\n",
    "        if names:\n",
    "            name = names[0]\n",
    "            return responses[\"user_Intro\"][0].format(name=name)\n",
    "        else:\n",
    "            return random.choice(responses[\"greeting\"])\n",
    "    elif \"what\" in query_lemmas and \"is\" in query_lemmas and \"my\" in query_lemmas and \"name\" in query_lemmas:\n",
    "        if name:\n",
    "            return responses[\"my_name\"][0].format(name=name)\n",
    "        else:\n",
    "            return \"I'm sorry, I didn't catch your name. Can you please tell me your name?\"\n",
    "    elif any(lemma in query_lemmas for lemma in booking_synonym(query)):\n",
    "        return responses[\"booking\"][0]\n",
    "    elif \"where\" in query_lemmas or (\"which\" in query_lemmas and \"country\"in query_lemmas):\n",
    "        return responses[\"available-country\"][0]\n",
    "    elif country_names:\n",
    "        country_name = country_names[0]\n",
    "        if country_name not in asean_countries:\n",
    "            return responses[\"not_understand\"][0].format(country_name=country_name) \n",
    "        else:\n",
    "            for country in asean_countries:\n",
    "                if country == country_name:\n",
    "                    return responses[\"country_name\"][0].format(country_name=country_name)\n",
    "    else:\n",
    "        return responses[\"not_understand\"][1]\n",
    "        #return \"I'm sorry, I didn't understand your query. Please try again.\"\n",
    "\n",
    "\n",
    "\n",
    "# Define a function to handle user interaction\n",
    "'''\n",
    "def chatbot():\n",
    "    print(\"Welcome to our customer service chatbot!\")\n",
    "    while True:\n",
    "        query = input(\"You: \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        response = chatbot_response(query)\n",
    "        print(\"Chatbot: \" + response)\n",
    "        #print(person_names)\n",
    "\n",
    "# Run the chatbot function\n",
    "chatbot()\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
